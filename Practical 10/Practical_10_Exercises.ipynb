{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left\">\n",
    "    <h1 style=\"width:450px\">Practical 10: Grouping Data</h1>\n",
    "    <h2 style=\"width:450px\">Classification &amp; Clusters</h2>\n",
    "</div>\n",
    "<div style=\"float:right\"><img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common challenge in data analysis is how to group observations in a data set together in a way that allows for generalisation: _this_ group of observations are similar to one another, _that_ group is dissimilar to this group. But what defines similarity and difference? There is no _one_ answer to that question and so there are many different ways to cluster data, each of which has strengths and weaknesses that make them more, or less, appropriate in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "import os\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# All of these are potentially useful, though\n",
    "# not all have been used in this practical --\n",
    "# I'd suggest exploring the use of different \n",
    "# Scalers/Transformers as well as clustering \n",
    "# algorithms...\n",
    "import sklearn\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PowerTransformer, RobustScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
    "from esda.adbscan import ADBSCAN\n",
    "\n",
    "import random\n",
    "random.seed(42)    # For reproducibility\n",
    "np.random.seed(42) # For reproducibility\n",
    "\n",
    "# Make numeric display a bit neater\n",
    "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise the Scalers and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbs = RobustScaler(quantile_range=[0.025,0.975])\n",
    "mms = MinMaxScaler(feature_range=(-1,1))\n",
    "pts = PowerTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Functions for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_ldn(w, b):\n",
    "    fig, ax = plt.subplots(1, figsize=(14, 12))\n",
    "    w.plot(ax=ax, color='#79aef5', zorder=2)\n",
    "    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n",
    "    ax.set_xlim([502000,563000])\n",
    "    ax.set_ylim([155000,201500])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    return fig, ax\n",
    "\n",
    "########################\n",
    "# These may no longer be relevant because of changes to geopandas API\n",
    "\n",
    "def default_cmap(n, outliers=False):\n",
    "    cmap = mpl.cm.get_cmap('viridis_r', n)\n",
    "    colors = cmap(np.linspace(0,1,n))\n",
    "    if outliers:\n",
    "        gray = np.array([225/256, 225/256, 225/256, 1])\n",
    "        colors = np.insert(colors, 0, gray, axis=0)\n",
    "    return ListedColormap(colors)\n",
    "\n",
    "# mappable = ax.collections[-1] if you add the geopandas\n",
    "# plot last.\n",
    "def add_colorbar(mappable, ax, cmap, norm, breaks, outliers=False):\n",
    "    cb = fig.colorbar(mappable, ax=ax, cmap=cmap, norm=norm,\n",
    "                    boundaries=breaks,\n",
    "                    extend=('min' if outliers else 'neither'), \n",
    "                    spacing='uniform',\n",
    "                    orientation='horizontal',\n",
    "                    fraction=0.05, shrink=0.5, pad=0.05)\n",
    "    cb.set_label(\"Cluster Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Water and Boroughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Water GeoPackage\n",
    "w_path = os.path.join('data','geo','Water.gpkg')\n",
    "if not os.path.exists(w_path):\n",
    "    water = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Water.gpkg?raw=true')\n",
    "    water.to_file(w_path)\n",
    "    print(\"Downloaded Water.gpkg file.\")\n",
    "else:\n",
    "    water = gpd.read_file(w_path)\n",
    "\n",
    "# Boroughs GeoPackage\n",
    "b_path = os.path.join('data','geo','Boroughs.gpkg')\n",
    "if not os.path.exists(b_path):\n",
    "    boroughs = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Boroughs.gpkg?raw=true')\n",
    "    boroughs.to_file(b_path)\n",
    "    print(\"Downloaded Boroughs.gpkg file.\")\n",
    "else:\n",
    "    boroughs = gpd.read_file(b_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the MSOA GeoData\n",
    "\n",
    "You should have this locally..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msoas = gpd.read_file(os.path.join('data','geo','London_MSOAs.gpkg'), driver='GPKG')\n",
    "msoas = msoas.drop(columns=['OBJECTID','MSOA11NM','BNG_E','BNG_N','Borough','msoa11hclnm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Listings Data\n",
    "\n",
    "Feel free to load your local copy rather than loading via the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/jreades/i2p/blob/master/data/clean/2020-08-24-listings.csv.gz?raw=true'\n",
    "df = pd.read_csv(url, compression='gzip', low_memory=False, \n",
    "                 usecols=['room_type','longitude','latitude','price'])\n",
    "print(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have: `Data frame is 74,120 x 4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(df, \n",
    "      geometry=gpd.points_from_xy(?, ?, crs=?))\n",
    "gdf = gdf.to_crs(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot(column='price', cmap='plasma', scheme='quantiles', k=10, markersize=1, figsize=(8,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate Listings by MSOA\n",
    "\n",
    "Next, use a spatial join to link the listings to MSOAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml == MSOA Listings\n",
    "ml = gpd.sjoin(??).drop(\n",
    "                    columns=['latitude','longitude','index_right']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want a count of room types by MSOA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlgc = ml.groupby([??])['price'].agg(Count='??').reset_index() # msoa listings grouped counts\n",
    "mlgc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get: \n",
    "\n",
    "|    | MSOA11CD | room_type | Count |\n",
    "| -: | :------- | :-------- | ----: |\n",
    "| 0 | E02000001 | Entire home/apt | 398 |\n",
    "| 1 | E02000001 | Hotel room | 4 |\n",
    "| 2 | E02000001 | Private room | 48 |\n",
    "| 3 | E02000001 | Shared room | 2 |\n",
    "| 4 | E02000002 | Private room | 10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's also get the median price by MSOA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlgp = ml.groupby('MSOA11CD')['price'].agg(??).reset_index() # msoa listings grouped price\n",
    "mlgp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison purposes: \n",
    "\n",
    "|    | MSOA11CD | price |\n",
    "| -: | :------- | ----: |\n",
    "| 0 | E02000001 | 125.00 |\n",
    "| 1 | E02000002 | 50.00 |\n",
    "| 2 | E02000003 | 40.00 | \n",
    "| 3 | E02000004 | 29.00 |\n",
    "| 4 | E02000005 | 69.00 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest median price is $1,880, which implies not just heavy skew but something a bit out-of-this-world.\n",
    "\n",
    "<div style=\"border: dotted 1px rgb(156,121,26); padding: 10px; margin: 5px; background-color: rgb(255,236,184)\"><i>Warning</i>: I have a strong suspicion that price is <i>so</i> skewed that using it in most clustering contexts is almost impossible. This is why I use the PowerTransformer below.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make use of the pivot table function to generate counts by MSOA in 'wide' format. I'd suggest using the empty cell below to work out the pivot table function first before assigning it back to the `mlgc` variable and overwriting the existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlgc.pivot(index=??, columns=??, values=??).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    | MSOA11CD |       |       |       | Count |\n",
    "| -: | :------- | ----: | ----: | ----: | ----: |\n",
    "| **room_type** | | **Entire home/apt** | **Hotel room** | **Private room** | **Shared room** |\n",
    "| 0 | E02000001 | 398.00 | 4.00 | 48.00 | 2.00 | \n",
    "| 1 | E02000002 | nan | nan | 10.00 | nan |\n",
    "| 2 | E02000003 | 5.00 | nan | 19.00 | nan |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlgc = mlgc.pivot().reset_index()\n",
    "mlgc.columns = ['MSOA11CD','Entire home/apt','Hotel room','Private room','Shared room']\n",
    "mlgc.drop(columns=['Hotel room','Shared room'], inplace=True)\n",
    "mlgc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the grouped counts and price back to a `mlg` (MSOA Listings Grouped) data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlg = pd.merge(mlgc, mlgp, on=??)\n",
    "mlg = mlg.fillna(0).set_index('MSOA11CD')\n",
    "mlg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a PowerTransform to every column using a loop and `fit_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in mlg.columns.values:\n",
    "    mlg[c] = ??\n",
    "mlg.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison:\n",
    "\n",
    "|      | Entire home/apt | Private room | price |\n",
    "| :--- | --------------: | -----------: | ----: |\n",
    "| **MSOA11CD** |  |  |  |\n",
    "| E02000001 | 2.12 | 0.81 | 1.81 | \n",
    "| E02000002 | -1.99 | -0.76 | -0.21 | \n",
    "| E02000003| -0.66 | -0.16 | -0.83 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "sns.pairplot(mlg, markers=\".\", height=3, diag_kind='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Reduced Dimension Data from Week 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddf = gpd.read_file(os.path.join('data','clean','Reduced_Dimension_Data.gpkg'), driver='GPKG').set_index('MSOA11CD')\n",
    "rddf.drop(columns=['OBJECTID','MSOA11NM','BNG_E','BNG_N','Borough','msoa11hclnm','Subregion',\n",
    "                   'geometry','Dimension 1','Dimension 2'], inplace=True)\n",
    "print(f\"Reduced Dimensionality Data Frame is {rddf.shape[0]:,} x {rddf.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `Reduced Dimensionality Data Frame is 983 x 10`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the `rddf` and `mlg` files using the MSOA11CD and rescale all columns using MinMaxScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the reducded dimensionality data frame with the msoa listings grouped data frame\n",
    "cldf = pd.merge(??)\n",
    "\n",
    "# Rescale the columns using the MinMaxScaler set up in the Preamble\n",
    "for c in cldf.columns:\n",
    "    cldf[c] = ??\n",
    "\n",
    "# We also create a 'result set' (rs) data frame to hold the \n",
    "# results of the clustering operation\n",
    "rs   = pd.merge(msoas, cldf, left_on='MSOA11CD', right_index=True).set_index('MSOA11CD')\n",
    "\n",
    "# Check output\n",
    "cldf.head(2)[['Component 1','Component 2','price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partial sample: \n",
    "\n",
    "| MSOA11CD | Component 1 | Component 2 | price |\n",
    "| :------- | ----------: | ----------: | ----: |\n",
    "| E02000001 | -0.08 | 0.12 | 0.25 |\n",
    "| E02000002 | -0.88 | -0.33 | -0.42 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select columns to plot\n",
    "\n",
    "Plotting all columns is computationally costly and for little utility... it's better to just select a few at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = np.random.choice(cldf.columns.values, 4, replace=False)\n",
    "print(cols_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_nm   = 'KMeans' # Clustering name\n",
    "k_pref = 3 # Number of clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf) # The process\n",
    "\n",
    "print(kmeans.labels_) # The results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now capture the labels (i.e. clusters) and write them to a data series that we store on the result set df (`rs`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add it to the data frame\n",
    "rs[c_nm] = pd.Series(kmeans.labels_, index=cldf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How are the clusters distributed?\n",
    "ax = sns.histplot(data=rs, x=c_nm, bins=k_pref);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to be a bit hard to read if \n",
    "# we plot every variable against every\n",
    "# other variables, so we'll just pick a few\n",
    "sns.set(style=\"white\")\n",
    "sns.pairplot(rs, \n",
    "             vars=cols_to_plot, \n",
    "             hue=c_nm, markers=\".\", height=3, diag_kind='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a map!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt_ldn(water, boroughs)\n",
    "fig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\n",
    "rs.plot(column=??, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: dotted 1px red; padding: 10px; margin: 5px; background-color: rgb(255,249,249);\"><i>Stop</i>: What critical assumption did we make when running this analysis?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 'Right' Number of Clusters\n",
    "\n",
    "There's more than one way to find the 'right' number of clusters. In Singleton's _Geocomputation_ chapter they use WCSS to pick the 'optimal' number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2...n) and then look for a 'knee' (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don't always see a knee.\n",
    "\n",
    "Another way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the 'quality' of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it's based on Partitioning Around the Medoid (PAM).\n",
    "\n",
    "Either way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results...\n",
    "\n",
    "Let's try clustering across a wider range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for k in range(??,??):\n",
    "    \n",
    "    #############\n",
    "    # Do the clustering using the main columns\n",
    "    kmeans = KMeans(??).fit(cldf)\n",
    "    \n",
    "    # Calculate the overall silhouette score\n",
    "    silhouette_avg = silhouette_score(cldf, kmeans.labels_)\n",
    "    \n",
    "    y.append(k)\n",
    "    x.append(silhouette_avg)\n",
    "    \n",
    "    print('.', end='')\n",
    "\n",
    "print()\n",
    "print(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n",
    "\n",
    "plt.plot(y, x)\n",
    "plt.gcf().suptitle(\"Average Silhouette Scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the largest average silhouette score to determine the 'natural' number of clusters in the data, but that that's only if we don't have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value... Again, we're now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Clustering \n",
    "\n",
    "So although we should probably pick the largest silhouette scores, that's `k=2` which kind of defeats the purpose of clustering in the first place. In the absence of a _compelling_ reason to pick 2 or 3 clusters, let's have a closer look at the _next_ maximum silhouetted score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_pref=??\n",
    "    \n",
    "#############\n",
    "# Do the clustering using the main columns\n",
    "kmeans = KMeans(??).fit(cldf)\n",
    "\n",
    "# Convert to a series\n",
    "s = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\n",
    "rs[c_nm] = s\n",
    "    \n",
    "# Calculate the overall silhouette score\n",
    "silhouette_avg = silhouette_score(cldf, kmeans.labels_)\n",
    "\n",
    "# Calculate the silhouette values\n",
    "sample_silhouette_values = silhouette_samples(cldf, kmeans.labels_)\n",
    "    \n",
    "#############\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(9, 5)\n",
    "\n",
    "# The 1st subplot is the silhouette plot\n",
    "# The silhouette coefficient can range from -1, 1\n",
    "ax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n",
    "    \n",
    "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "# plots of individual clusters, to demarcate them clearly.\n",
    "ax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n",
    "    \n",
    "y_lower = 10\n",
    "    \n",
    "# For each of the clusters...\n",
    "for i in range(k_pref):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = \\\n",
    "        sample_silhouette_values[kmeans.labels_ == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "    # Set the color ramp\n",
    "    color = plt.cm.Spectral(i/k)\n",
    "    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                        0, ith_cluster_silhouette_values,\n",
    "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed --\n",
    "    # we can only do this for the first two dimensions\n",
    "    # so we may not see fully what is causing the \n",
    "    # resulting assignment\n",
    "    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k)\n",
    "    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], \n",
    "                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"Visualization of the clustered data\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "plt.suptitle((\"Silhouette results for KMeans clustering \"\n",
    "                \"with %d clusters\" % k_pref),\n",
    "                fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: dotted 1px red; padding: 10px; margin: 5px; background-color: rgb(255,249,249);\"><i>Stop</i>: Make sure that you understand how the silhouette plot and value work, and why your results may diverge from mine..</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt_ldn(water, boroughs)\n",
    "fig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\n",
    "rs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'Representative' Centroids\n",
    "\n",
    "To get a sense of how these clusters differ we can try to extract 'representative' centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There's also a k-medoids clustering approach built around the median centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = None\n",
    "for k in sorted(rs[c_nm].unique()):\n",
    "    print(f\"Processing cluster {k}\")\n",
    "\n",
    "    c = rs[rs[c_nm]==k]\n",
    "    if centroids is None:\n",
    "        centroids = pd.DataFrame(columns=c.columns.values)\n",
    "    centroids = centroids.append(c.mean(), ignore_index=True)\n",
    "    \n",
    "odf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\n",
    "for i in range(0,len(centroids.index)):\n",
    "    row = centroids.iloc[i,:]\n",
    "    c_index = list(centroids.columns.values).index(c_nm)\n",
    "    for c in range(0,c_index):\n",
    "        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n",
    "        odf = odf.append(d, ignore_index=True)\n",
    "\n",
    "odf = odf[~odf.Variable.isin(['Borough','msoa11hclnm','Subregion','geometry'])]\n",
    "g = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\n",
    "g = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "For what it's worth, I've had _enormous_ trouble with DBSCAN and this kind of data. I don't think it deals very well with more than three dimensions, so the flexbility to not have to specify the number of clusters is balanced with a density-based approach that is severely hampered by high-dimensional distance-inflation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cldf2 = cldf.loc[:,['Component 1','Component 2']]\n",
    "cldf2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work out the Neighbour Distance\n",
    "\n",
    "We normally look for some kind of 'knee' to set the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=2).fit(cldf2)\n",
    "distances, indices = nbrs.kneighbors(cldf2)\n",
    "\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)\n",
    "plt.gcf().suptitle(\"Nearest Neighbour Distances\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration\n",
    "\n",
    "There are two values that need to be specified: `eps` and `min_samples`. Both seem to be set largely by trial and error. It's easiest to set `min_samples` first since that sets a floor for your cluster size and then `eps` is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster.\n",
    "\n",
    "<div style=\"padding:5px;margin-top:5px;margin-bottom:5px;border:dotted 1px red;background-color:rgb(255,233,233);color:red\">WARNING. This next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn't) the graph above.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_nm = 'DBSCAN'\n",
    "\n",
    "# Make numeric display a bit neater\n",
    "pd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n",
    "\n",
    "el  = []\n",
    "\n",
    "max_clusters  = 10\n",
    "cluster_count = 1\n",
    "\n",
    "iters = 0\n",
    "\n",
    "for e in np.arange(0.025, 0.25, 0.005):\n",
    "    \n",
    "    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n",
    "    \n",
    "    # Run the clustering\n",
    "    dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2)\n",
    "    \n",
    "    # See how we did\n",
    "    s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n",
    "    \n",
    "    row = [e]\n",
    "    data = s.value_counts()\n",
    "    \n",
    "    for c in range(-1, max_clusters+1):\n",
    "        try:\n",
    "            if np.isnan(data[c]):\n",
    "                row.append(None)\n",
    "            else: \n",
    "                row.append(data[c])\n",
    "        except KeyError:\n",
    "            row.append(None)\n",
    "    \n",
    "    el.append(row)\n",
    "    iters+=1\n",
    "\n",
    "edf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n",
    "\n",
    "# Make numeric display a bit neater\n",
    "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n",
    "\n",
    "for i in range(0,len(edf.index)):\n",
    "    row = edf.iloc[i,:]\n",
    "    for c in range(1,len(edf.columns.values)):\n",
    "        if row[c] != None and not np.isnan(row[c]):\n",
    "            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n",
    "            odf = odf.append(d, ignore_index=True)\n",
    "\n",
    "odf['Count'] = odf.Count.astype(float)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster');\n",
    "ax.vlines(0.06, 0, ax.get_ylim()[1], linestyle='dashed', color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0.06\n",
    "dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2.values)\n",
    "s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n",
    "rs[c_nm] = s\n",
    "print(s.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping Clustering Results\n",
    "\n",
    "<div style=\"padding:5px;margin-top:5px;margin-bottom:5px;border:dotted 1px red;background-color:rgb(255,233,233);color:red\">WARNING. My sense is that these results are a bit rubbish: the majority of items are assigned to <i>one cluster</i>??? I've tried PCA on the standardised data and that made little difference. This should also have worked <i>better</i> but it seems that a small number of LSOAs are so utterly different that the more sophisticated clustering algorithm effectively 'chokes' on them.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt_ldn(water, boroughs)\n",
    "fig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n",
    "rs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = None\n",
    "for k in sorted(rs[c_nm].unique()):\n",
    "    print(f\"Processing cluster {k}\")\n",
    "\n",
    "    clsoas = rs[rs[c_nm]==k]\n",
    "    if centroids is None:\n",
    "        centroids = pd.DataFrame(columns=clsoas.columns.values)\n",
    "    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n",
    "\n",
    "odf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\n",
    "for i in range(0,len(centroids.index)):\n",
    "    row = centroids.iloc[i,:]\n",
    "    c_index = list(centroids.columns.values).index(c_nm)\n",
    "    for c in range(0,c_index):\n",
    "        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n",
    "        odf = odf.append(d, ignore_index=True)\n",
    "\n",
    "# Drop outliers\n",
    "odf = odf[odf.Cluster >= 0]\n",
    "odf.drop(index=odf[odf.Variable.isin(['geometry','KMeans'])].index, inplace=True)\n",
    "\n",
    "g = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\n",
    "g = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n",
    "\n",
    "del(odf, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus!\n",
    "\n",
    "See if you can work out why we have this issue by plotting the first 2 components... We _could_ actually try clusteirng on secondary components instead (e.g. 2 and 3, or 3 and 4) on the basis that the first one is so dominant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTICS\n",
    "\n",
    "This one invites you to experiment wih different values of $\\epsilon$ so it makes even fewer assumptions than DBSCAN but is even more computationally intensive. I've deliberately left this untested so I can't be sure it will work and you'll need to debug..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:5px;margin-top:5px;margin-bottom:5px;border:dotted 1px red;background-color:rgb(255,233,233);color:red\">WARNING. This next step may take quite a lot of time since the algorithm is making far fewer assumptions about the structure of the data. On a 2018 MacBook Pro with 16GB of RAM it took about 5 minutes.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_nm = 'Optics'\n",
    "\n",
    "# Can try to set this from DBSCAN results\n",
    "e = 0.06\n",
    "\n",
    "import math\n",
    "\n",
    "# Run the clustering\n",
    "opt = OPTICS(min_samples=len(cldf2.columns)+1, max_eps=math.ceil(e * 100)/10, n_jobs=-1).fit(cldf2)\n",
    "\n",
    "# See how we did\n",
    "s = pd.Series(opt.labels_, index=cldf.index, name=c_nm)\n",
    "rs[c_nm] = s\n",
    "\n",
    "# Distribution\n",
    "print(s.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Organising Maps\n",
    "\n",
    "SOMs offer a _fourth_ type of clustering algorithm. They are a relatively 'simple' type of neural network in which the 'map' (of the SOM) adjusts to the data: we're going to see how this works over the next few code blocks, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data. In this sense there is a conceptual link between SOMs and PCA or tSNE (another form of dimensionality reduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the SOM\n",
    "\n",
    "We are going to actually train the SOM using the input data. This is where you specify the input parameters that have the main effect on the clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy.sompy import SOMFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_nm = 'SOM'\n",
    "\n",
    "sm = SOMFactory().build(\n",
    "    cldf.values, mapsize=(10,15),\n",
    "    normalization='var', initialization='random', component_names=cldf.columns.values)\n",
    "sm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is the fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topographic_error  = sm.calculate_topographic_error()\n",
    "quantization_error = np.mean(sm._bmu[1])\n",
    "print(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the results look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy.visualization.mapview import View2D\n",
    "view2D = View2D(10, 10, \"rand data\", text_size=10)\n",
    "view2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\n",
    "plt.savefig(f\"{c_nm}-Map.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's What I Got\n",
    "\n",
    "<div style=\"border: dotted 1px rgb(156,121,26); padding: 10px; margin: 5px; background-color: rgb(255,236,184)\"><i>Note</i>: Your results may differ.</div>\n",
    "\n",
    "<img src=\"https://github.com/jreades/i2p/blob/master/practicals/img/SOM-Map.png?raw=true\" alt=\"SOM Clustering Results\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many data points were assigned to each BMU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy.visualization.bmuhits import BmuHitsView\n",
    "vhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\n",
    "vhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\n",
    "plt.savefig(f\"{c_nm}-BMU Hit View.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many clusters do we want and where are they on the map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sompy.visualization.hitmap import HitMapView\n",
    "\n",
    "k_val = 6 # The way this library is set up it's hard to explore the k-means clustering of Hits\n",
    "sm.cluster(k_val)\n",
    "hits  = HitMapView(15, 15, \"Clustering\", text_size=14)\n",
    "a     = hits.show(sm)\n",
    "plt.savefig(f\"{c_nm}-Hit Map View.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's get the cluster results and map them back on to the data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for each BMU\n",
    "# in the SOM (15 * 10 neurons)\n",
    "clabs = sm.cluster_labels\n",
    "\n",
    "# Project the data on to the SOM\n",
    "# so that we get the BMU for each\n",
    "# of the original data points \n",
    "# (This is similar to the transform \n",
    "# function is sklearn)\n",
    "bmus  = sm.project_data(cldf.values)\n",
    "\n",
    "# Turn the BMUs into cluster labels\n",
    "# and append to the data frame\n",
    "s = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n",
    "\n",
    "rs[c_nm] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt_ldn(water, boroughs)\n",
    "fig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n",
    "rs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result!\n",
    "\n",
    "<div style=\"padding:5px;margin-top:5px;margin-bottom:5px;border:dotted 1px red;background-color:rgb(255,233,233);color:red\">WARNING. These are the results from the approach that is closest to the one outlined in <i>Geocomputation</i>.</div>\n",
    "\n",
    "<img src=\"https://github.com/jreades/i2p/blob/master/practicals/img/SOM-Cluster-Map.png?raw=true\" alt=\"SOM Clustering Results Mapped\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADBSCAN\n",
    "\n",
    "https://pysal.org/esda/generated/esda.adbscan.ADBSCAN.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the appropriate eps value: [Nearest Neighbour Distance Functions](https://nbviewer.jupyter.org/github/pysal/pointpats/blob/master/notebooks/distance_statistics.ipynb#Nearest-Neighbor-Distance-Functions) or [Interevent Distance Functions](https://nbviewer.jupyter.org/github/pysal/pointpats/blob/master/notebooks/distance_statistics.ipynb#Interevent-Distance-Functions)\n",
    "- [Clustering Points](https://darribas.org/gds_course/content/bH/lab_H.html#clusters-of-points)\n",
    "- [Regionalisation algorithms with Aglomerative Clustering](https://darribas.org/gds_course/content/bG/lab_G.html#regionalization-algorithms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've reached the end, you're done... \n",
    "\n",
    "Er, no. This is barely scratching the surface! I'd suggest that you go back through the above code and do three things:\n",
    "1. Add a lot more comments to the code to ensure that really have understood what is going on.\n",
    "2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change.\n",
    "3. Try outputting additional plots that will help you to understand the _quality_ of your clustering results (e.g. what _is_ the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\n",
    "\n",
    "If all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\n",
    "\n",
    "See: [Introduction to Machine Learning with Scikit-Learn](http://www.slideshare.net/BenjaminBengfort/introduction-to-machine-learning-with-scikitlearn)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
